{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/vianne/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda2/lib/python2.7/site-packages/sklearn/ensemble/weight_boosting.py:29: DeprecationWarning: numpy.core.umath_tests is an internal NumPy module and should not be imported. It will be removed in a future NumPy release.\n",
      "  from numpy.core.umath_tests import inner1d\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import string\n",
    "import math\n",
    "import glob\n",
    "import os\n",
    "import itertools\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.cm as cm\n",
    "from PIL import Image\n",
    "from IPython.display import display, Markdown\n",
    "import nltk\n",
    "nltk.download('stopwords')\n",
    "from nltk.stem import PorterStemmer\n",
    "from nltk.corpus import stopwords\n",
    "from nltk import word_tokenize, pos_tag\n",
    "from nltk.stem.wordnet import WordNetLemmatizer\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfTransformer\n",
    "from sklearn import svm\n",
    "from sklearn import preprocessing\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.neighbors import NearestNeighbors as KNN\n",
    "from sklearn.neighbors import DistanceMetric\n",
    "from sklearn.ensemble import RandomForestRegressor,RandomForestClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.cross_decomposition import PLSRegression,PLSCanonical,CCA\n",
    "from scipy import misc\n",
    "import re\n",
    "import csv\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Read Image features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get feature\n",
    "train_feat_fc, test_feat_fc = [], []\n",
    "for line in open(\"./data_final/features_train/features_resnet1000_train.csv\"):\n",
    "    l = line.strip().split(\",\")\n",
    "    train_feat_fc.append(l)\n",
    "train_feat_fc = np.array(train_feat_fc)\n",
    "for line in open(\"./data_final/features_test/features_resnet1000_test.csv\"):\n",
    "    l = line.strip().split(\",\")\n",
    "    test_feat_fc.append(l)\n",
    "test_feat_fc = np.array(test_feat_fc)\n",
    "train_feat_p, test_feat_p = [], []\n",
    "for line in open(\"./data_final/features_train/features_resnet1000intermediate_train.csv\"):\n",
    "    l = line.strip().split(\",\")\n",
    "    train_feat_p.append(l)\n",
    "train_feat_p = np.array(train_feat_p)\n",
    "for line in open(\"./data_final/features_test/features_resnet1000intermediate_test.csv\"):\n",
    "    l = line.strip().split(\",\")\n",
    "    test_feat_p.append(l)\n",
    "test_feat_p = np.array(test_feat_p)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_feat_fc_dict, test_feat_fc_dict,train_feat_p_dict, test_feat_p_dict = dict(),dict(),dict(),dict()\n",
    "def get_feat_dict(feat,feat_dict):\n",
    "    for i in range(len(feat)):\n",
    "        name = int(feat[i][0].split(\"/\")[1].split(\".\")[0])\n",
    "        feat_dict[name] = np.array(feat[i][1:], dtype=float)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "get_feat_dict(train_feat_fc,train_feat_fc_dict)\n",
    "get_feat_dict(test_feat_fc,test_feat_fc_dict)\n",
    "get_feat_dict(train_feat_p,train_feat_p_dict)\n",
    "get_feat_dict(test_feat_p,test_feat_p_dict)\n",
    "\n",
    "# img features in: train_feat_fc_dict, test_feat_fc_dict,train_feat_p_dict, test_feat_p_dict\n",
    "# key: sample name, val: feature"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Descriptions to Bag of words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#analyze words in descriptions and create a word dictionary \n",
    "des_word_dict = dict()\n",
    "for filename in glob.glob(os.path.join(\"./data_final/descriptions_train/\", \"*.txt\")):\n",
    "    with open(filename, \"r\") as des_file:\n",
    "        des = des_file.read()\n",
    "        des = np.char.lower(des)\n",
    "        des = re.sub('[^\\w\\s]', ' ', str(des))\n",
    "        stemming = PorterStemmer()\n",
    "        for word in des.split():\n",
    "            try:\n",
    "                if word not in stopwords.words(\"english\"):\n",
    "                    w = stemming.stem(word)\n",
    "                    if w in des_word_dict:\n",
    "                        des_word_dict[w] += 1\n",
    "                    else:\n",
    "                        des_word_dict[w] = 1\n",
    "            except:\n",
    "                pass\n",
    "# des_word_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a BoW model for train and test descriptions\n",
    "word_dict = dict()\n",
    "ind = 0\n",
    "for word in des_word_dict:\n",
    "    if des_word_dict[word] >= 2:\n",
    "        word_dict[word] = ind\n",
    "        ind += 1\n",
    "\n",
    "def get_bow_des(path,word_dict):\n",
    "    des_vec = dict()\n",
    "    for filename in glob.glob(os.path.join(path, \"*.txt\")):\n",
    "        with open(filename, \"r\") as des_file:\n",
    "            des = des_file.read()\n",
    "            des = np.char.lower(des)\n",
    "            des = re.sub('[^\\w\\s]', ' ', str(des))\n",
    "            stemming = PorterStemmer()\n",
    "            sent_bow = [0] * len(word_dict)\n",
    "            for word in des.split():\n",
    "                try:\n",
    "                    if word not in stopwords.words(\"english\"):\n",
    "                        w = stemming.stem(word)\n",
    "                        if w in word_dict:\n",
    "                            sent_bow[word_dict[w]] += 1\n",
    "                except:\n",
    "                    pass\n",
    "            des_vec[int(os.path.splitext(filename.split('/')[-1])[0])] = sent_bow\n",
    "    return des_vec\n",
    "\n",
    "train_des = get_bow_des(\"./data_final/descriptions_train/\",word_dict)\n",
    "test_des = get_bow_des(\"./data_final/descriptions_test/\",word_dict)\n",
    "\n",
    "# word_dict\n",
    "# key: word, value: indec\n",
    "\n",
    "#train_des, test_des\n",
    "#key: sample name, value: bow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_samplename = np.array(train_des.keys())\n",
    "test_samplename = np.array(test_des.keys())\n",
    "\n",
    "# train_samplename,test_samplename\n",
    "# sample names in order"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "#TF-IDF transformation\n",
    "\n",
    "xTr_des = np.array([train_des[i] for i in train_samplename])\n",
    "xTe_des = np.array([test_des[i] for i in test_samplename])\n",
    "transformer_des = TfidfTransformer(smooth_idf=False)\n",
    "\n",
    "xTr_des_norm = transformer_des.fit_transform(xTr_des).toarray()\n",
    "xTe_des_norm = transformer_des.transform(xTe_des).toarray()\n",
    "\n",
    "# xTr_des_norm, xTe_des_norm\n",
    "# bow matrix after tfidf, order corresponds to train_samplename,test_samplename"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Generated:\n",
    "    xTr_des_norm, xTe_des_norm (description input)\n",
    "    train_samplename, test_samplename (file name/label)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tag-based prediction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tags to Bag of words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "#analyze words in tags and create a word dictionary \n",
    "tag_word_dict = dict()\n",
    "for filename in glob.glob(os.path.join(\"./data_final/tags_train/\", \"*.txt\")):\n",
    "    with open(filename, \"r\") as tag_file:\n",
    "        tag = tag_file.read()\n",
    "        tag = np.char.lower(tag)\n",
    "        tag = re.sub('[^\\w\\s]', ' ', str(tag))\n",
    "        tag = re.sub(':', ' ', str(tag))\n",
    "        stemming = PorterStemmer()\n",
    "        for word in tag.split():\n",
    "            try:\n",
    "                if word not in stopwords.words(\"english\"): #not stop word\n",
    "                    if nltk.pos_tag([word])[0][1] in ['NN','NNP','NNS','NNPS']: #is noun\n",
    "                        w = stemming.stem(word)\n",
    "                        if w in tag_word_dict:\n",
    "                            tag_word_dict[w] += 1\n",
    "                        else:\n",
    "                            tag_word_dict[w] = 1\n",
    "            except:\n",
    "                pass\n",
    "\n",
    "# create a BoW model for train and test descriptions\n",
    "tag_dict = dict()\n",
    "ind = 0\n",
    "for word in tag_word_dict:\n",
    "    if tag_word_dict[word] >=2 :\n",
    "        tag_dict[word] = ind\n",
    "        ind += 1\n",
    "        \n",
    "def get_bow_tag(path,tag_dict):\n",
    "    tag_vec = dict()\n",
    "    for filename in glob.glob(os.path.join(path, \"*.txt\")):\n",
    "        with open(filename, \"r\") as tag_file:\n",
    "            tag = tag_file.read()\n",
    "            tag = np.char.lower(tag)\n",
    "            tag = re.sub('[^\\w\\s]', ' ', str(tag))\n",
    "            stemming = PorterStemmer()\n",
    "            tag_bow = [0] * len(tag_dict)\n",
    "            for word in tag.split():\n",
    "                try:\n",
    "                    if word not in stopwords.words(\"english\"):\n",
    "                        w = stemming.stem(word)\n",
    "                        if w in tag_dict:\n",
    "                            tag_bow[tag_dict[w]] = 1\n",
    "                except:\n",
    "                    pass\n",
    "            tag_vec[int(os.path.splitext(filename.split('/')[-1])[0])] = tag_bow\n",
    "    return tag_vec\n",
    "train_tag = get_bow_tag(\"./data_final/tags_train/\",tag_dict)\n",
    "test_tag = get_bow_tag(\"./data_final/tags_test/\",tag_dict)\n",
    "\n",
    "# tag_dict\n",
    "# key: word, values: indec\n",
    "\n",
    "#train_tag, test_tag\n",
    "# key: sample, values: bow_tags"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_tagname = np.array(train_tag.keys())\n",
    "test_tagname = np.array(test_tag.keys())\n",
    "\n",
    "#sample names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "#TF-IDF transformation\n",
    "\n",
    "xTr_tag = np.array([train_tag[i] for i in train_tagname])\n",
    "xTe_tag = np.array([test_tag[i] for i in test_tagname])\n",
    "#transformer_tag = TfidfTransformer(smooth_idf=False)\n",
    "\n",
    "#xTr_tag_norm = transformer_tag.fit_transform(xTr_tag).toarray()\n",
    "#xTe_tag_norm = transformer_tag.transform(xTe_tag).toarray()\n",
    "\n",
    "# xTr_tag, xTe_tag\n",
    "# bow_tag matrix with order in train_tagname, test_tagname"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Description_bow to tags_bow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Linear SVC to predict tags from description\n",
    "svm_yTe = []\n",
    "for i in range(len(train_tag[4000])):\n",
    "    y = xTr_tag[:,i]\n",
    "    clf = svm.LinearSVC(random_state=0, tol=1e-5,C=2)\n",
    "#     clf = svm.SVC(random_state=0, tol=1e-5,C=1)\n",
    "    clf.fit(xTr_des_norm, y)\n",
    "    tag = clf.predict(xTe_des_norm)\n",
    "    svm_yTe.append(tag)\n",
    "svm_yTe = np.array(svm_yTe).T\n",
    "\n",
    "# svm_yTe: predicted tags for each description"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Predicted tags to img tags using kNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test descriptions: test_samplename (y)\n",
    "# test tags: test_tagname\n",
    "knn = KNN(n_neighbors=20,metric='mahalanobis',metric_params={'V': np.cov(xTe_tag,rowvar=False),'VI':np.linalg.pinv(np.cov(xTe_tag,rowvar=False))})\n",
    "knn = knn.fit(xTe_tag,test_tagname)\n",
    "tag_predictions = knn.kneighbors(svm_yTe, return_distance = False)\n",
    "tag_predictions = test_tagname[tag_predictions]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "knn_dist = KNN(n_neighbors=2000,metric='mahalanobis',metric_params={'V': np.cov(xTe_tag,rowvar=False),'VI':np.linalg.pinv(np.cov(xTe_tag,rowvar=False))})\n",
    "knn_dist = knn_dist.fit(xTe_tag,test_tagname)\n",
    "tag_predictions_dist = knn_dist.kneighbors(svm_yTe, return_distance = True)\n",
    "tag_similarity = tag_predictions_dist[0]\n",
    "\n",
    "#tag_predictions_dist = test_tagname[tag_predictions_dist[1]]\n",
    "nn_2000_tag = tag_predictions_dist[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def write_dist(test_samplename,predictions,nn_2000,output='p5_no_noun_distance.csv'):\n",
    "#     sorted_dist = []\n",
    "#     for i in range(2000):\n",
    "#         sorted_samplename, sorted_predictions = zip(*sorted(zip(nn_2000[i,:],predictions[i,:])))\n",
    "#         sorted_dist.append(sorted_predictions)\n",
    "    \n",
    "#     nn_list = []\n",
    "#     for i, row in enumerate(sorted_dist):\n",
    "#         temp = str(test_samplename[i])\n",
    "#         for j, val in enumerate(row):\n",
    "#             temp = temp + ' ' + str(val)\n",
    "#         nn_list.append(temp)\n",
    "#     index = []\n",
    "#     for i in sorted_samplename:\n",
    "#         index.append(str(i))\n",
    "#     with open(output, 'wb') as f:\n",
    "#         writer = csv.writer(f)\n",
    "#         writer.writerows(itertools.izip(index, nn_list))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# write_result(test_samplename,tag_predictions,output='final_tags_1205.csv')\n",
    "# write_dist(test_samplename,tag_similarity,nn_2000_tag,'tag_distance.csv')\n",
    "# sorted_tag_similarity = []\n",
    "# for i in range(2000):\n",
    "#     t_sorted_samplename, t_sorted_predictions = zip(*sorted(zip(nn_2000_tag[i,:],tag_similarity[i,:])))\n",
    "#     sorted_tag_similarity.append(t_sorted_predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print bow for debugging\n",
    "# print(test_samplename[0])\n",
    "# print(test_des.keys()[0])\n",
    "\n",
    "# for ind in (np.where(np.array(test_des[str(test_des.keys()[0])])!=0)[0]):\n",
    "#     print(np.array(word_dict.keys())[np.where(np.array(word_dict.values()==ind))[0]])\n",
    "# print(\"====\")\n",
    "# for ind in (np.where(td_tree_yTe[0]!=0)[0]):\n",
    "#     print(tag_dict.keys()[ind])\n",
    "# print(\"====\")\n",
    "# for ind in (np.where(xTe_tag_norm[0]!=0)[0]):\n",
    "#     print(tag_dict.keys()[ind])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Image-based prediction\n",
    "## Image Pool5 features to Bag of words (all words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "#image feature from pool5 to bow_all\n",
    "p5Tr = []\n",
    "p5Te = []\n",
    "desTr = xTr_des_norm\n",
    "desTe = xTe_des_norm\n",
    "yTr = train_samplename\n",
    "yTe = test_samplename\n",
    "for i in train_samplename:\n",
    "    p5Tr.append(train_feat_p_dict[i])\n",
    "\n",
    "for j in test_samplename:\n",
    "    p5Te.append(test_feat_p_dict[j])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda2/lib/python2.7/site-packages/sklearn/cross_decomposition/pls_.py:77: UserWarning: Maximum number of iterations reached\n",
      "  warnings.warn('Maximum number of iterations reached')\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "PLSRegression(copy=True, max_iter=500, n_components=400, scale=True,\n",
       "       tol=1e-06)"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pls2 = PLSRegression(n_components=400)\n",
    "pls2.fit(p5Tr, desTr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "pls2_pred = pls2.predict(p5Te)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "#find knn cosine dist\n",
    "yTe = np.array(yTe)\n",
    "# test descriptions: test_samplename (y)\n",
    "# test tags: test_tagname\n",
    "knn_cos = KNN(n_neighbors=20,metric='cosine')\n",
    "knn_cos = knn_cos.fit(pls2_pred,yTe)\n",
    "\n",
    "img_predictions = knn_cos.kneighbors(desTe, return_distance = False)\n",
    "img_predictions = yTe[img_predictions]\n",
    "\n",
    "###\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "knn_img_dist = KNN(n_neighbors=2000,metric='cosine')\n",
    "knn_img_dist = knn_img_dist.fit(pls2_pred,yTe)\n",
    "img_predictions_dist = knn_img_dist.kneighbors(desTe, return_distance = True)\n",
    "#####\n",
    "img_similarity = img_predictions_dist[0]\n",
    "nn_2000_img =img_predictions_dist[1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ensembling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "# write_result(test_samplename,img_predictions,output='final_pool5_pls_allwords_results.csv')\n",
    "# write_dist(test_samplename,img_similarity,nn_2000_img,'pool5_pls_all_distance.csv')\n",
    "# sorted_img_similarity = []\n",
    "# for i in range(2000):\n",
    "#     t_sorted_samplename, t_sorted_predictions = zip(*sorted(zip(nn_2000_img[i,:],img_similarity[i,:])))\n",
    "#     sorted_img_similarity.append(t_sorted_predictions)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def write_result(test_samplename,predictions,output='tune_result_bowall.csv'):\n",
    "#     sorted_test_samplename, sorted_predictions = zip(*sorted(zip(test_samplename,predictions)))\n",
    "#     nn_list = []\n",
    "#     for i, row in enumerate(sorted_predictions):\n",
    "#         temp = ''\n",
    "#         for j, val in enumerate(row):\n",
    "#             temp = temp + ' ' + (str(val) + \".jpg\")\n",
    "#         nn_list.append(temp)\n",
    "#     index = []\n",
    "#     for i in sorted_test_samplename:\n",
    "#         index.append(str(i)+ \".txt\")\n",
    "#     with open(output, 'wb') as f:\n",
    "#         writer = csv.writer(f)\n",
    "#         writer.writerows(itertools.izip(index, nn_list))\n",
    "# def get_acc(test_samplename,predictions):\n",
    "#     acc = 0\n",
    "#     score = 0\n",
    "#     for i in range(len(test_samplename)):\n",
    "#         if test_samplename[i] in predictions[i]:\n",
    "#             acc+=1\n",
    "#             score += float(20-np.where(predictions[i]==test_samplename[i])[0])/20\n",
    "#     print(float(score)/len(test_samplename))\n",
    "#     print(acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from sklearn.preprocessing import scale\n",
    "# from sklearn.preprocessing import MinMaxScaler,minmax_scale"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "# #scaler = MinMaxScaler()\n",
    "# scaled_img_sim,scaled_tag_sim = [],[]\n",
    "# for row in sorted_img_similarity:\n",
    "#     scaled_img_sim.append(minmax_scale(np.array(row)))\n",
    "# for rov in sorted_tag_similarity:\n",
    "#     scaled_tag_sim.append(minmax_scale(np.array(rov)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ensemble_sim = np.add(np.asarray(scaled_img_sim), np.asarray(scaled_tag_sim))\n",
    "# ensemble_nn = np.argsort(ensemble_sim, axis=1)\n",
    "# write_result(test_samplename,ensemble_nn[:,0:20],output='ensemble_tag_pls_all_v1.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fc_data = np.loadtxt(\"distance/distance_pls_400_fc.csv\", delimiter=',', usecols=range(1,2001))\n",
    "# #scaler = MinMaxScaler()\n",
    "# scaled_fc_sim = []\n",
    "# for rou in fc_data:\n",
    "#     scaled_fc_sim.append(minmax_scale(np.array(rou)))\n",
    "# ensemble_sim2 = np.add(ensemble_sim, np.asarray(scaled_fc_sim))\n",
    "# ensemble_nn2 = np.argsort(ensemble_sim2, axis=1)\n",
    "# write_result(test_samplename,ensemble_nn2[:,0:20],output='ensemble_tag_plsall_fc_v2.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [],
   "source": [
    "# p5nn_data = np.loadtxt(\"distance/pool5_pls_no_nouns_cosinedistance.csv\", delimiter=' ', usecols=range(1,2001))\n",
    "# scaled_p5nn_sim = []\n",
    "# for rox in p5nn_data:\n",
    "#     scaled_p5nn_sim.append(minmax_scale(np.array(rox)))\n",
    "# ensemble_sim3 = np.add(ensemble_sim2, np.asarray(scaled_p5nn_sim))\n",
    "# ensemble_nn3 = np.argsort(ensemble_sim3, axis=1)\n",
    "# write_result(test_samplename,ensemble_nn3[:,0:20],output='ensemble_tag_plsall_plsnn_fc_v3.csv')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [],
   "source": [
    "tag2_data = np.loadtxt(\"distance/distance_pls_100_tag.csv\", delimiter=',', usecols=range(1,2001))\n",
    "p750_data = np.loadtxt(\"distance/distance_pls_750_pool.csv\", delimiter=',', usecols=range(1,2001))\n",
    "scaled_tag2_sim = []\n",
    "for r1 in tag2_data:\n",
    "    scaled_tag2_sim.append(minmax_scale(np.array(r1)))\n",
    "scaled_p750_sim = []\n",
    "for r2 in p750_data:\n",
    "    scaled_p750_sim.append(minmax_scale(np.array(r2)))\n",
    "\n",
    "    \n",
    "ensemble_sim4 = np.add(0.5*np.asarray(scaled_tag_sim), np.asarray(scaled_fc_sim))\n",
    "ensemble_sim4 = np.add(ensemble_sim4, 0.5*np.asarray(scaled_tag2_sim))\n",
    "ensemble_sim4 = np.add(ensemble_sim4, np.asarray(scaled_p750_sim))\n",
    "\n",
    "ensemble_nn4 = np.argsort(ensemble_sim4, axis=1)\n",
    "\n",
    "write_result(test_samplename,ensemble_nn4[:,0:20],output='ensemble_tag_plsall_plstag_fc_v7.csv')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
